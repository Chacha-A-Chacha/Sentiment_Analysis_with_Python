{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project Steps\n",
    "\n",
    "## 1. Define Project Scope and Objectives\n",
    "   - Clearly define the goals and scope of the sentiment analysis project.\n",
    "   - Identify the target audience and the specific use case for sentiment analysis.\n",
    "\n",
    "## 2. Data Acquisition\n",
    "   - Identify and collect data sources relevant to the project.\n",
    "   - Utilize APIs, web scraping, or pre-existing datasets for data retrieval.\n",
    "   - Ensure data sources align with the project objectives.\n",
    "\n",
    "```python\n",
    "# Example Code for Data Acquisition using Tweepy (for social media data)\n",
    "import tweepy\n",
    "\n",
    "# Set up Twitter API credentials\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Collect tweets based on a specific hashtag\n",
    "tweets = api.search(q='#StockMarket', count=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m client \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mClient(bearer_token\u001b[38;5;241m=\u001b[39mbearer_token)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Collect tweets based on a specific hashtag\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_recent_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#StockMarket\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/client.py:1266\u001b[0m, in \u001b[0;36mClient.search_recent_tweets\u001b[0;34m(self, query, user_auth, **params)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;124;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/2/tweets/search/recent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedia.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoll.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_order\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muntil_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    126\u001b[0m ):\n\u001b[1;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[0;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/client.py:100\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal."
     ]
    }
   ],
   "source": [
    "# Example Code for Data Acquisition using Tweepy (for social media data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "\n",
    "# Collect tweets based on a specific hashtag\n",
    "tweets = client.search_recent_tweets(query='#StockMarket', max_results=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Exploration\n",
    "\n",
    "    Explore and analyze the collected data to understand its structure and characteristics.\n",
    "    Identify any patterns or anomalies in the data that may impact sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Data Exploration\n",
    "\n",
    "# Convert tweets to a DataFrame for exploration\n",
    "df = pd.DataFrame([tweet.text for tweet in tweets], columns=['Tweet'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Cleaning and Preprocessing\n",
    "\n",
    "    Handle missing values, duplicates, and outliers.\n",
    "    Perform text cleaning, including removing special characters, links, and unnecessary whitespace.\n",
    "    Tokenize text and remove stop words to prepare data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Data Cleaning and Preprocessing\n",
    "\n",
    "\n",
    "# Function for text cleaning and preprocessing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    \n",
    "     # Remove special characters and numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "\n",
    "    \n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply text cleaning to the 'Tweet' column\n",
    "df['CleanedTweet'] = df['Tweet'].apply(clean_text)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sentiment Analysis Tools Selection\n",
    "\n",
    "Sentiment analysis is a crucial step in understanding the emotional tone of text data. Choosing the right sentiment analysis tool is essential for accurate and meaningful results. Here are some popular tools and libraries for sentiment analysis:\n",
    "    \n",
    "    5.1 NLTK (Natural Language Toolkit)\n",
    "\n",
    "        Description:\n",
    "            NLTK is a powerful library for natural language processing (NLP) in Python.\n",
    "            It provides various tools for tasks like tokenization, stemming, and sentiment analysis.\n",
    "\n",
    "        Pros:\n",
    "            Widely used and well-documented.\n",
    "            Offers flexibility for customizing analysis pipelines.\n",
    "\n",
    "        Cons:\n",
    "            May require additional resources for specific sentiment lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code using NLTK for Sentiment Analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment of a sentence\n",
    "sentence = \"NLTK is a fantastic library for NLP.\"\n",
    "sentiment_score = sia.polarity_scores(sentence)\n",
    "print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 TextBlob\n",
    "\n",
    "    Description:\n",
    "        TextBlob is a simplified NLP library built on top of NLTK.\n",
    "        It provides an easy-to-use API for common NLP tasks, including sentiment analysis.\n",
    "\n",
    "    Pros:\n",
    "        Beginner-friendly with a simple API.\n",
    "        Performs well for general sentiment analysis tasks.\n",
    "\n",
    "    Cons:\n",
    "        May not be as customizable as lower-level libraries like NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code using TextBlob for Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Analyze sentiment of a sentence\n",
    "sentence = \"TextBlob makes NLP tasks easy.\"\n",
    "analysis = TextBlob(sentence)\n",
    "print(analysis.sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "\n",
    "    Description:\n",
    "        VADER is a pre-built sentiment analysis tool designed for social media text.\n",
    "        It is specifically tuned to handle sentiments expressed in short texts like tweets.\n",
    "\n",
    "    Pros:\n",
    "        Out-of-the-box solution for social media sentiment analysis.\n",
    "        Handles nuances like emoticons and slang well.\n",
    "\n",
    "    Cons:\n",
    "        May not be as accurate for longer, more complex texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code using VADER for Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment of a sentence\n",
    "sentence = \"VADER is great for social media analysis!\"\n",
    "sentiment_score = sia.polarity_scores(sentence)\n",
    "print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Visualization\n",
    "   - Create visualizations to present sentiment analysis results.\n",
    "   - Utilize tools like Matplotlib, Seaborn, or interactive dashboards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
